{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe69i79LsnnyLrkCzCoPIN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashraj01-se/NLP-with-Deep-Learning/blob/main/n_gram(practical).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "8uKsB9py1-mO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X_I3etJw1u8y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data and Preprocessing"
      ],
      "metadata": {
        "id": "XwpNJ3tn2Fat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Vocabulary:\n",
        "corpous=\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse,\n",
        "and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\n",
        "It is a way I have of driving off the spleen and regulating the circulation.\n",
        "Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul;\n",
        "whenever I find myself involuntarily pausing before coffin warehouses, and bringing up the rear of every funeral I meet;\n",
        "and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me\n",
        "from deliberately stepping into the street, and methodically knocking people’s hats off—then, I account it high time\n",
        "to get to sea as soon as I can.\n",
        "\"\"\"\n",
        "words=corpous.split()\n",
        "vocab=list(set(words))\n",
        "vocab_size=len(vocab)\n",
        "word2idx={w:i for i,w in enumerate(vocab)}\n",
        "idx2word={i:w for w,i in word2idx.items()}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uK-mMuUT2E-m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Function defining pair for n gram:\n",
        "pairs=[]\n",
        "def pair_maker(text,n):\n",
        "  for i in range(len(text)-n+1):\n",
        "    context=text[i:i+n-1]\n",
        "    target=text[i+n-1]\n",
        "    pairs.append(([word2idx[w] for w in context],word2idx[target]))\n",
        "\n",
        "pair_maker(vocab,3)\n",
        "print(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4PuGMDA3TN_",
        "outputId": "7aacaa34-cf34-4bc3-dcfc-d3981776d3ed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[([0, 1], 2), ([1, 2], 3), ([2, 3], 4), ([3, 4], 5), ([4, 5], 6), ([5, 6], 7), ([6, 7], 8), ([7, 8], 9), ([8, 9], 10), ([9, 10], 11), ([10, 11], 12), ([11, 12], 13), ([12, 13], 14), ([13, 14], 15), ([14, 15], 16), ([15, 16], 17), ([16, 17], 18), ([17, 18], 19), ([18, 19], 20), ([19, 20], 21), ([20, 21], 22), ([21, 22], 23), ([22, 23], 24), ([23, 24], 25), ([24, 25], 26), ([25, 26], 27), ([26, 27], 28), ([27, 28], 29), ([28, 29], 30), ([29, 30], 31), ([30, 31], 32), ([31, 32], 33), ([32, 33], 34), ([33, 34], 35), ([34, 35], 36), ([35, 36], 37), ([36, 37], 38), ([37, 38], 39), ([38, 39], 40), ([39, 40], 41), ([40, 41], 42), ([41, 42], 43), ([42, 43], 44), ([43, 44], 45), ([44, 45], 46), ([45, 46], 47), ([46, 47], 48), ([47, 48], 49), ([48, 49], 50), ([49, 50], 51), ([50, 51], 52), ([51, 52], 53), ([52, 53], 54), ([53, 54], 55), ([54, 55], 56), ([55, 56], 57), ([56, 57], 58), ([57, 58], 59), ([58, 59], 60), ([59, 60], 61), ([60, 61], 62), ([61, 62], 63), ([62, 63], 64), ([63, 64], 65), ([64, 65], 66), ([65, 66], 67), ([66, 67], 68), ([67, 68], 69), ([68, 69], 70), ([69, 70], 71), ([70, 71], 72), ([71, 72], 73), ([72, 73], 74), ([73, 74], 75), ([74, 75], 76), ([75, 76], 77), ([76, 77], 78), ([77, 78], 79), ([78, 79], 80), ([79, 80], 81), ([80, 81], 82), ([81, 82], 83), ([82, 83], 84), ([83, 84], 85), ([84, 85], 86), ([85, 86], 87), ([86, 87], 88), ([87, 88], 89), ([88, 89], 90), ([89, 90], 91), ([90, 91], 92), ([91, 92], 93), ([92, 93], 94), ([93, 94], 95), ([94, 95], 96), ([95, 96], 97), ([96, 97], 98)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 2. Parameters ----\n",
        "embed_size=10\n",
        "hidden_size=10\n",
        "n=3\n",
        "W_embed = torch.randn(vocab_size, embed_size, requires_grad=True)\n",
        "W_hidden = torch.randn((n-1)*embed_size, hidden_size, requires_grad=True)\n",
        "b_hidden = torch.zeros(hidden_size, requires_grad=True)\n",
        "W_out = torch.randn(hidden_size, vocab_size, requires_grad=True)\n",
        "b_out = torch.zeros(vocab_size, requires_grad=True)\n",
        "\n",
        "# ---- 3. Training ----\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD([W_embed, W_hidden, b_hidden, W_out, b_out], lr=0.05)\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context_idxs, target_idx in pairs:\n",
        "        # ---- Forward ----\n",
        "        embeds = W_embed[context_idxs].view(1, -1)  # flatten context embeddings\n",
        "        h = torch.tanh(embeds @ W_hidden + b_hidden)  # hidden layer\n",
        "        logits = h @ W_out + b_out                   # output layer\n",
        "        loss = loss_fn(logits,torch.tensor([target_idx]))\n",
        "\n",
        "        # ---- Backward ----\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ---- 4. Evaluation ----\n",
        "print(\"\\nN-gram Predictions:\")\n",
        "for context_idxs, _ in pairs:\n",
        "    context_words = [idx2word[i] for i in context_idxs]\n",
        "    embeds = W_embed[context_idxs].view(1, -1)\n",
        "    h = torch.tanh(embeds @ W_hidden + b_hidden)\n",
        "    logits = h @ W_out + b_out\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    top_idx = torch.argmax(probs, dim=1).item()\n",
        "    predicted_word = idx2word[top_idx]\n",
        "    print(f\"Context: {context_words} -> Predicted: {predicted_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV2XQDY06rFa",
        "outputId": "61665d7c-4cde-4200-9d4f-1620108c081f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/200, Loss: 100.2392\n",
            "Epoch 40/200, Loss: 41.8500\n",
            "Epoch 60/200, Loss: 26.0132\n",
            "Epoch 80/200, Loss: 17.8003\n",
            "Epoch 100/200, Loss: 12.5954\n",
            "Epoch 120/200, Loss: 9.7913\n",
            "Epoch 140/200, Loss: 8.0767\n",
            "Epoch 160/200, Loss: 6.8754\n",
            "Epoch 180/200, Loss: 5.9983\n",
            "Epoch 200/200, Loss: 5.3364\n",
            "\n",
            "N-gram Predictions:\n",
            "Context: ['regulating', 'off—then,'] -> Predicted: way\n",
            "Context: ['off—then,', 'way'] -> Predicted: precisely—having\n",
            "Context: ['way', 'precisely—having'] -> Predicted: to\n",
            "Context: ['precisely—having', 'to'] -> Predicted: from\n",
            "Context: ['to', 'from'] -> Predicted: particular\n",
            "Context: ['from', 'particular'] -> Predicted: that\n",
            "Context: ['particular', 'that'] -> Predicted: account\n",
            "Context: ['that', 'account'] -> Predicted: little\n",
            "Context: ['account', 'little'] -> Predicted: high\n",
            "Context: ['little', 'high'] -> Predicted: would\n",
            "Context: ['high', 'would'] -> Predicted: part\n",
            "Context: ['would', 'part'] -> Predicted: people’s\n",
            "Context: ['part', 'people’s'] -> Predicted: Call\n",
            "Context: ['people’s', 'Call'] -> Predicted: my\n",
            "Context: ['Call', 'my'] -> Predicted: meet;\n",
            "Context: ['my', 'meet;'] -> Predicted: upper\n",
            "Context: ['meet;', 'upper'] -> Predicted: November\n",
            "Context: ['upper', 'November'] -> Predicted: requires\n",
            "Context: ['November', 'requires'] -> Predicted: drizzly\n",
            "Context: ['requires', 'drizzly'] -> Predicted: circulation.\n",
            "Context: ['drizzly', 'circulation.'] -> Predicted: mouth;\n",
            "Context: ['circulation.', 'mouth;'] -> Predicted: an\n",
            "Context: ['mouth;', 'an'] -> Predicted: spleen\n",
            "Context: ['an', 'spleen'] -> Predicted: about\n",
            "Context: ['spleen', 'about'] -> Predicted: rear\n",
            "Context: ['about', 'rear'] -> Predicted: deliberately\n",
            "Context: ['rear', 'deliberately'] -> Predicted: strong\n",
            "Context: ['deliberately', 'strong'] -> Predicted: no\n",
            "Context: ['strong', 'no'] -> Predicted: in\n",
            "Context: ['no', 'in'] -> Predicted: every\n",
            "Context: ['in', 'every'] -> Predicted: me\n",
            "Context: ['every', 'me'] -> Predicted: years\n",
            "Context: ['me', 'years'] -> Predicted: sea\n",
            "Context: ['years', 'sea'] -> Predicted: stepping\n",
            "Context: ['sea', 'stepping'] -> Predicted: driving\n",
            "Context: ['stepping', 'driving'] -> Predicted: world.\n",
            "Context: ['driving', 'world.'] -> Predicted: such\n",
            "Context: ['world.', 'such'] -> Predicted: as\n",
            "Context: ['such', 'as'] -> Predicted: hats\n",
            "Context: ['as', 'hats'] -> Predicted: growing\n",
            "Context: ['hats', 'growing'] -> Predicted: up\n",
            "Context: ['growing', 'up'] -> Predicted: whenever\n",
            "Context: ['up', 'whenever'] -> Predicted: Some\n",
            "Context: ['whenever', 'Some'] -> Predicted: soon\n",
            "Context: ['Some', 'soon'] -> Predicted: watery\n",
            "Context: ['soon', 'watery'] -> Predicted: pausing\n",
            "Context: ['watery', 'pausing'] -> Predicted: and\n",
            "Context: ['pausing', 'and'] -> Predicted: mind\n",
            "Context: ['and', 'mind'] -> Predicted: especially\n",
            "Context: ['mind', 'especially'] -> Predicted: thought\n",
            "Context: ['especially', 'thought'] -> Predicted: principle\n",
            "Context: ['thought', 'principle'] -> Predicted: coffin\n",
            "Context: ['principle', 'coffin'] -> Predicted: moral\n",
            "Context: ['coffin', 'moral'] -> Predicted: ago—never\n",
            "Context: ['moral', 'ago—never'] -> Predicted: money\n",
            "Context: ['ago—never', 'money'] -> Predicted: is\n",
            "Context: ['money', 'is'] -> Predicted: involuntarily\n",
            "Context: ['is', 'involuntarily'] -> Predicted: into\n",
            "Context: ['involuntarily', 'into'] -> Predicted: find\n",
            "Context: ['into', 'find'] -> Predicted: knocking\n",
            "Context: ['find', 'knocking'] -> Predicted: how\n",
            "Context: ['knocking', 'how'] -> Predicted: have\n",
            "Context: ['how', 'have'] -> Predicted: it\n",
            "Context: ['have', 'it'] -> Predicted: shore,\n",
            "Context: ['it', 'shore,'] -> Predicted: I\n",
            "Context: ['shore,', 'I'] -> Predicted: grim\n",
            "Context: ['I', 'grim'] -> Predicted: soul;\n",
            "Context: ['grim', 'soul;'] -> Predicted: prevent\n",
            "Context: ['soul;', 'prevent'] -> Predicted: time\n",
            "Context: ['prevent', 'time'] -> Predicted: nothing\n",
            "Context: ['time', 'nothing'] -> Predicted: or\n",
            "Context: ['nothing', 'or'] -> Predicted: damp,\n",
            "Context: ['or', 'damp,'] -> Predicted: get\n",
            "Context: ['damp,', 'get'] -> Predicted: hand\n",
            "Context: ['get', 'hand'] -> Predicted: the\n",
            "Context: ['hand', 'the'] -> Predicted: methodically\n",
            "Context: ['the', 'methodically'] -> Predicted: Ishmael.\n",
            "Context: ['methodically', 'Ishmael.'] -> Predicted: a\n",
            "Context: ['Ishmael.', 'a'] -> Predicted: before\n",
            "Context: ['a', 'before'] -> Predicted: can.\n",
            "Context: ['before', 'can.'] -> Predicted: interest\n",
            "Context: ['can.', 'interest'] -> Predicted: funeral\n",
            "Context: ['interest', 'funeral'] -> Predicted: off\n",
            "Context: ['funeral', 'off'] -> Predicted: bringing\n",
            "Context: ['off', 'bringing'] -> Predicted: Whenever\n",
            "Context: ['bringing', 'Whenever'] -> Predicted: street,\n",
            "Context: ['Whenever', 'street,'] -> Predicted: me,\n",
            "Context: ['street,', 'me,'] -> Predicted: long\n",
            "Context: ['me,', 'long'] -> Predicted: myself\n",
            "Context: ['long', 'myself'] -> Predicted: hypos\n",
            "Context: ['myself', 'hypos'] -> Predicted: warehouses,\n",
            "Context: ['hypos', 'warehouses,'] -> Predicted: on\n",
            "Context: ['warehouses,', 'on'] -> Predicted: purse,\n",
            "Context: ['on', 'purse,'] -> Predicted: sail\n",
            "Context: ['purse,', 'sail'] -> Predicted: of\n",
            "Context: ['sail', 'of'] -> Predicted: see\n",
            "Context: ['of', 'see'] -> Predicted: It\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "5ljuf2mnjTjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "sYjpkA9pi5mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data and Pre-Processing:"
      ],
      "metadata": {
        "id": "ZZL20XfAjMvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0wwNuQTJc7lX"
      },
      "outputs": [],
      "source": [
        "#1.Corpus and Word Processing:\n",
        "corpous=\"the quick brown fox jumped over the lazy dog\"\n",
        "words=corpous.split()\n",
        "vocab=set(words)\n",
        "word2inx={w:i for i , w in enumerate(vocab)}\n",
        "indx2word={i:w for w,i in word2inx.items()}\n",
        "Vocab_size=len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Skip-gram pairs:\n",
        "def generate_pairs(text, window_size=2):\n",
        "    pairs = []\n",
        "    for i, word in enumerate(text):\n",
        "        for j in range(max(0, i - window_size), min(len(text), i + window_size + 1)):\n",
        "            if i != j:\n",
        "                pairs.append((word, text[j]))\n",
        "    return pairs\n",
        "\n",
        "pairs = generate_pairs(words, window_size=2)\n",
        "pairs_idx = [(word2inx[t], word2inx[c]) for t, c in pairs]"
      ],
      "metadata": {
        "id": "35W_XzgLi8rN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec Model:"
      ],
      "metadata": {
        "id": "lUTBXfwgjX3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Model:\n",
        "from ctypes import BigEndianStructure\n",
        "class Word2Vec_Model(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_size):\n",
        "    super().__init__()\n",
        "    self.embedding=nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_size)\n",
        "    self.weight=nn.Linear(in_features=embed_size,out_features=vocab_size,bias=False)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.embedding(x)\n",
        "    logits=self.weight(x)\n",
        "    return logits\n",
        "\n",
        "model=Word2Vec_Model(vocab_size=Vocab_size,embed_size=10)"
      ],
      "metadata": {
        "id": "bVuphsLPjbIJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loss Function and Optimizer:"
      ],
      "metadata": {
        "id": "E9fWnq1ZmALG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Loss function and optimizer:\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(params=model.parameters(),\n",
        "                           lr=0.02)"
      ],
      "metadata": {
        "id": "6lT-C2LAl_od"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training loop:"
      ],
      "metadata": {
        "id": "udSWk90Sn4RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=500\n",
        "for epoch in range(epochs):\n",
        "  total_loss=0\n",
        "  for target,context in pairs_idx:\n",
        "    target=torch.tensor(data=target,dtype=torch.long)\n",
        "    context=torch.tensor(data=context,dtype=torch.long)\n",
        "\n",
        "    logits=model(target)\n",
        "    loss=loss_fn(logits,context)\n",
        "    total_loss+=loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  if (epoch%100==0):\n",
        "    print(f\"Epoch: {epoch} and loss : {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_MaZnPSle7W",
        "outputId": "ec380504-617e-47c5-b081-053b40d603c0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 and loss : 42.2553\n",
            "Epoch: 100 and loss : 42.3063\n",
            "Epoch: 200 and loss : 42.2900\n",
            "Epoch: 300 and loss : 42.1870\n",
            "Epoch: 400 and loss : 42.2290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing Loop:"
      ],
      "metadata": {
        "id": "UDfFpi6_tUVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  for word in vocab:\n",
        "    target=torch.tensor([word2inx[word]],dtype=torch.long)\n",
        "    embedding=model.embedding.weight[word2inx[word]].numpy()\n",
        "\n",
        "    logits=model(target)\n",
        "    probs=torch.softmax(logits,dim=1)\n",
        "    top_idx=torch.argmax(probs,dim=1).item()\n",
        "    predicted_word=indx2word[top_idx]\n",
        "\n",
        "    print(f\"Word : {word}\")\n",
        "    print(f\"Embeddings : {probs}\")\n",
        "    print(f\"Predicted word: {predicted_word}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMBlM5Azq2HQ",
        "outputId": "36de2b6f-f1ae-45b5-ded0-bc16781cdd50"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word : fox\n",
            "Embeddings : tensor([[0.0011, 0.2575, 0.2453, 0.2614, 0.0004, 0.2317, 0.0011, 0.0016]])\n",
            "Predicted word: over\n",
            "\n",
            "Word : jumped\n",
            "Embeddings : tensor([[2.4897e-01, 3.2452e-04, 2.4295e-01, 2.5161e-01, 2.5414e-01, 4.2492e-04,\n",
            "         1.4436e-04, 1.4426e-03]])\n",
            "Predicted word: the\n",
            "\n",
            "Word : brown\n",
            "Embeddings : tensor([[0.2498, 0.2576, 0.0004, 0.0011, 0.2354, 0.2536, 0.0015, 0.0007]])\n",
            "Predicted word: jumped\n",
            "\n",
            "Word : over\n",
            "Embeddings : tensor([[2.4319e-01, 2.5174e-01, 1.4718e-03, 2.4187e-05, 2.5265e-01, 1.7330e-03,\n",
            "         2.4888e-01, 3.0818e-04]])\n",
            "Predicted word: the\n",
            "\n",
            "Word : the\n",
            "Embeddings : tensor([[0.0019, 0.1700, 0.1510, 0.1698, 0.0029, 0.1509, 0.1775, 0.1761]])\n",
            "Predicted word: lazy\n",
            "\n",
            "Word : quick\n",
            "Embeddings : tensor([[3.3963e-01, 7.4940e-05, 3.3603e-01, 2.1535e-03, 3.2115e-01, 1.8873e-06,\n",
            "         9.2807e-04, 3.3287e-05]])\n",
            "Predicted word: fox\n",
            "\n",
            "Word : lazy\n",
            "Embeddings : tensor([[1.9867e-03, 1.0247e-04, 1.8728e-03, 3.3425e-01, 3.3980e-01, 1.0723e-03,\n",
            "         3.3935e-04, 3.2058e-01]])\n",
            "Predicted word: the\n",
            "\n",
            "Word : dog\n",
            "Embeddings : tensor([[2.2287e-03, 4.5224e-04, 3.6493e-04, 3.0295e-06, 5.1328e-01, 1.3911e-06,\n",
            "         4.8295e-01, 7.1955e-04]])\n",
            "Predicted word: the\n",
            "\n"
          ]
        }
      ]
    }
  ]
}